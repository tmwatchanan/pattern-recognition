{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Mini Project 2</h1></center>\n",
    "<center><h2>Watchanan Chantapakul (<a href=\"mailto:wcgzm@umsystem.edu\"><code>wcgzm</code></a>)</h2></center>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import sympy\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "np.random.seed(7720)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two-class dataset has gaussian likelihood functions and priors $P(\\omega_1) = 4P(\\omega_2)$. Let the parameters of the likelihoods be $\\mu_1 = \\begin{pmatrix} 7 \\\\ 1 \\end{pmatrix}$, $\\mu_2 = \\begin{pmatrix} 1 \\\\ 7 \\end{pmatrix}$ and $\\Sigma_1 = \\Sigma_2 = \\begin{bmatrix} 3.1 & 0 \\\\ 0 & 2.6 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('P(ùúî1) = 4P(ùúî2)', True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 2 # number of classes\n",
    "d = 2 # dimension\n",
    "priors = [4/5, 1/5]\n",
    "\"P(ùúî1) = 4P(ùúî2)\", priors[0] == 4 * priors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mu1', array([7, 1]), 'mu2', array([1, 7]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = np.array([[7, 1], [1, 7]])\n",
    "\"mu1\", means[0], \"mu2\", means[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sigma1, Sigma2 =',\n",
       " array([[[3.1, 0. ],\n",
       "         [0. , 2.6]],\n",
       " \n",
       "        [[3.1, 0. ],\n",
       "         [0. , 2.6]]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = np.zeros((C, d, d))\n",
    "for c in range(C):\n",
    "    cov[c]= np.array([[3.1, 0], [0, 2.6]])\n",
    "\"Sigma1, Sigma2 =\", cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.random.multivariate_normal(mean=means[c], cov=cov[c], size=C)\n",
    "# print(\"x1\", x[0])\n",
    "# print(\"x2\", x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12, 6))\n",
    "# ax = fig.add_subplot(111)\n",
    "# viz_X1 = np.random.multivariate_normal(mean=means[0], cov=cov, size=1000)\n",
    "# viz_X2 = np.random.multivariate_normal(mean=means[1], cov=cov, size=1000)\n",
    "# ax.scatter(viz_X1[:, 0], viz_X1[:, 1], label=\"$\\omega_1$\")\n",
    "# ax.scatter(viz_X2[:, 0], viz_X2[:, 1], label=\"$\\omega_2$\")\n",
    "# ax.scatter(means[0, 0], means[0, 1], marker=\"*\", s=250, label=\"$\\mu_1$\")\n",
    "# ax.scatter(means[1, 0], means[1, 1], marker=\"*\", s=250, label=\"$\\mu_2$\")\n",
    "# ax.scatter(x[0, 0], x[0, 1], marker=\"x\", color=\"red\", s=100, label=\"Test sample 1\")\n",
    "# ax.scatter(x[1, 0], x[1, 1], marker=\"x\", color=\"magenta\", s=100, label=\"Test sample 2\")\n",
    "# ax.set_aspect(1)\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question A\n",
    "a) Write a generic Matlab function[<sup>1</sup>](#fn1) to compute the Mahalanobis distances between two arbitrary samples $\\vec{x_1}$ and $\\vec{x_2}$ or the distance between a sample $\\vec{x_1}$ and the center of any given Gaussian distribution with covariance $\\Sigma$, mean $\\mu$, and dimension $d$.\n",
    "\n",
    "<span id=\"fn1\"><sup>1</sup> you may use any computer language/package, but you may NOT use any function other than the basic operations: i.e. +, -, *, / (for scalars, vectors, or matrices)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "- $\\mathbf{A}$ = a matrix\n",
    "- $i$ = row\n",
    "- $j$ = column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $(i, j)$-Minor of a Matrix\n",
    "$$ \\mathbf{M}_{i, j} = \\operatorname{det}((\\mathbf{A}_{p, q})_{p \\neq i, q \\neq j}) = |(\\mathbf{A}_{p, q})_{p \\neq i, q \\neq j}| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submatrix(A, i, j):\n",
    "    B = []\n",
    "    for row in A[:i] + A[i+1:]:\n",
    "        B.append(row[:j]+ row[j+1:])\n",
    "    return B\n",
    "\n",
    "def minor(A, i, j):\n",
    "    return determinant(submatrix(A, i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy's delete == transpose(A) True\n",
      "------------------------------------\n",
      "np =\n",
      " [[1 3 1]\n",
      " [0 2 1]\n",
      " [0 4 2]]\n",
      "submatrix =\n",
      " [[1 3 1]\n",
      " [0 2 1]\n",
      " [0 4 2]]\n"
     ]
    }
   ],
   "source": [
    "A = [[1,3,1,4],[3,9,5,15],[0,2,1,1],[0,4,2,3]]\n",
    "i = 1\n",
    "j = 3\n",
    "np_submatrix = np.delete(np.delete(A,i,axis=0), j, axis=1)\n",
    "my_submatrix = np.array(submatrix(A, i, j))\n",
    "print(\"Numpy's delete == transpose(A)\", np.allclose(np_submatrix, my_submatrix))\n",
    "print(\"------------------------------------\")\n",
    "print(\"np =\\n\", np_submatrix)\n",
    "print(\"submatrix =\\n\", my_submatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant\n",
    "$$ \\operatorname{det}(A) = |\\mathbf{A}| = \\sum_{i=0, j} (-1)^{i+j} \\cdot \\mathbf{A}_{i, j} \\cdot \\mathbf{M}_{i, j} = \\sum_{i=0, j} (-1)^{j} \\cdot \\mathbf{A}_{0, j} \\cdot \\mathbf{M}_{0, j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinant(A):\n",
    "    if len(A) == 2:\n",
    "        det = (A[0][0] * A[1][1]) - (A[1][0] * A[0][1])\n",
    "        return det\n",
    "    else:\n",
    "        det = 0\n",
    "        \n",
    "        i = 0\n",
    "        for j in range(len(A)):\n",
    "            det += ((-1) ** (i + j)) * A[i][j] * minor(A, i, j)\n",
    "        return det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.lin.det(A) == determinant(A) True\n",
      "------------------------------------\n",
      "np = -3.999999999999999\n",
      "determinant = -4\n"
     ]
    }
   ],
   "source": [
    "A = [[1,3,1,4],[3,9,5,15],[0,2,1,1],[0,4,2,3]]\n",
    "\n",
    "np_det = np.linalg.det(A)\n",
    "my_det = determinant(A)\n",
    "print(\"np.lin.det(A) == determinant(A)\", np.allclose(np_det, my_det))\n",
    "print(\"------------------------------------\")\n",
    "print(\"np =\", np_det)\n",
    "print(\"determinant =\", my_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix of Cofactors\n",
    "$$ \\mathbf{C}_{i, j} = (‚Äì1)^{i+j}  \\mathbf{M}_{i, j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofactor(m, i, j):\n",
    "    return ((-1) ** (i + j)) * m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose\n",
    "$$ \\mathbf{A}^{\\mathsf{T}}_{i, j} = \\mathbf{A}_{j, i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(m):\n",
    "    return list(map(list,zip(*m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.transpose(A) == transpose(A) True\n",
      "------------------------------------\n",
      "np =\n",
      " [[ 1  3  0  0]\n",
      " [ 3  9  2  4]\n",
      " [ 1  5  1  2]\n",
      " [ 4 15  1  3]]\n",
      "transpose =\n",
      " [[ 1  3  0  0]\n",
      " [ 3  9  2  4]\n",
      " [ 1  5  1  2]\n",
      " [ 4 15  1  3]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,3,1,4],[3,9,5,15],[0,2,1,1],[0,4,2,3]])\n",
    "np_t = A.T\n",
    "my_t = np.array(transpose(A))\n",
    "print(\"np.transpose(A) == transpose(A)\", np.allclose(np_t, my_t))\n",
    "print(\"------------------------------------\")\n",
    "print(\"np =\\n\", np_t)\n",
    "print(\"transpose =\\n\", my_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjugate Matrix\n",
    "$$ \\operatorname{adj}(\\mathbf{A}) = \\mathbf{C}^{\\mathsf{T}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjugate(A):\n",
    "    N = len(A)\n",
    "    adj = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            adj[i, j] = cofactor(minor(A, i, j), i, j)\n",
    "    return transpose(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sympy's adjugate == adjugate(A) True\n",
      "------------------------------------\n",
      "sympy =\n",
      " [[ -1.  -1. -20.  13.]\n",
      " [ -3.   1.   0.  -1.]\n",
      " [  6.  -2. -12.   6.]\n",
      " [  0.   0.   8.  -4.]]\n",
      "adjugate =\n",
      " [[ -1.  -1. -20.  13.]\n",
      " [ -3.   1.   0.  -1.]\n",
      " [  6.  -2. -12.   6.]\n",
      " [  0.   0.   8.  -4.]]\n"
     ]
    }
   ],
   "source": [
    "A = [[1,3,1,4],[3,9,5,15],[0,2,1,1],[0,4,2,3]]\n",
    "sympy_adj = np.array(sympy.Matrix(A).adjugate(), dtype=float)\n",
    "my_adj = np.array(adjugate(A), dtype=float)\n",
    "\n",
    "print(\"Sympy's adjugate == adjugate(A)\", np.allclose(sympy_adj, my_adj))\n",
    "print(\"------------------------------------\")\n",
    "print(\"sympy =\\n\", sympy_adj)\n",
    "print(\"adjugate =\\n\", my_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Matrix\n",
    "$$ \\mathbf{A}^{-1} = |\\mathbf{A}|^{-1} \\cdot \\operatorname{adj}(\\mathbf{A}) = \\frac{\\operatorname{adj}(\\mathbf{A})}{|\\mathbf{A}|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_matrix(A):\n",
    "    if len(A) == 2:\n",
    "        inv_A = [\n",
    "            [A[1][1], -A[0][1]],\n",
    "            [-A[1][0], A[0][0]]\n",
    "        ]\n",
    "        return np.array(inv_A) / determinant(A)\n",
    "    else:\n",
    "        adj = adjugate(A)\n",
    "        return np.array(adj) / determinant(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.linalg.inv(A) == inverse_matrix(A) True\n",
      "------------------------------------------\n",
      "np.linalg.inv(A) =\n",
      " [[ 2.5000000e-01  2.5000000e-01  5.0000000e+00 -3.2500000e+00]\n",
      " [ 7.5000000e-01 -2.5000000e-01 -4.4408921e-16  2.5000000e-01]\n",
      " [-1.5000000e+00  5.0000000e-01  3.0000000e+00 -1.5000000e+00]\n",
      " [-0.0000000e+00 -0.0000000e+00 -2.0000000e+00  1.0000000e+00]]\n",
      "inverse_matrix(A) =\n",
      " [[ 0.25  0.25  5.   -3.25]\n",
      " [ 0.75 -0.25 -0.    0.25]\n",
      " [-1.5   0.5   3.   -1.5 ]\n",
      " [-0.   -0.   -2.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# A = [[1, 2], [3, 4]]\n",
    "A = [[1,3,1,4],[3,9,5,15],[0,2,1,1],[0,4,2,3]]\n",
    "np_inv = np.linalg.inv(A)\n",
    "my_inv = inverse_matrix(A)\n",
    "\n",
    "print(\"np.linalg.inv(A) == inverse_matrix(A)\", np.allclose(np_inv, my_inv))\n",
    "print(\"------------------------------------------\")\n",
    "print(\"np.linalg.inv(A) =\\n\", np_inv)\n",
    "print(\"inverse_matrix(A) =\\n\", my_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis Distance\n",
    "$$ r^2 = (\\vec{x} - \\vec{y})^{\\mathsf{T}} \\mathbf{\\Sigma}^{-1} (\\vec{x} - \\vec{y}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(x, mean, cov):\n",
    "    a = np.array(x) - np.array(mean)\n",
    "    r2 = a.T @ inverse_matrix(cov) @ a\n",
    "    return np.sqrt(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_mahalanobis = np.zeros((len(x), C))\n",
    "# scipy_mahalanobis = np.zeros_like(my_mahalanobis)\n",
    "# for i in range(len(x)):\n",
    "#     for c in range(C):\n",
    "#         my_mahalanobis[i, c] = mahalanobis_distance(x[i], means[c], cov[c])\n",
    "#         scipy_mahalanobis[i, c] = scipy.spatial.distance.mahalanobis(x[i], means[c], VI=np.linalg.inv(cov[c]))\n",
    "# print(my_mahalanobis)\n",
    "# print(scipy_mahalanobis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question B\n",
    "\n",
    "b) Write another Matlab function[<sup>1</sup>](#fn11) to call the function above and compute the discriminant function with the following generic form \n",
    "$$ g_i(x) = -\\frac{1}{2}(x-\\mu_i)^t\\Sigma_i^{-1}(x-\\mu_i)-\\frac{d}{2}\\ln (2\\pi) - \\frac{1}{2} \\ln |\\Sigma_i| + \\ln P(\\omega_i) $$\n",
    "also for any given $d$ dimensional data, mean, covariance matrix and prior probabilities.\n",
    "\n",
    "<span id=\"fn11\"><sup>1</sup> you may use any computer language/package, but you may NOT use any function other than the basic operations: i.e. +, -, *, / (for scalars, vectors, or matrices)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminant_fx(x, mean, cov, prior):\n",
    "    d = len(x)\n",
    "    return (-0.5 * mahalanobis_distance(x, mean, cov)) - ((d/2) * np.log(2 * np.pi)) - (0.5 * np.log(determinant(cov))) + np.log(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) write a Matlab program that generates (say, 1000) samples from the two classes with the parameters in part a); and plot the two classes in 3D. (your plot should be similar to figure 2.10 (b) in the\n",
    "textbook). The class samples above MUST be created from a Gaussian distribution with $N(~0, \\mathbf{I})$ (ie. use the concept of whitening in an inverse manner).[<sup>2</sup>](#fn2)\n",
    "\n",
    "<span id=\"fn2\"><sup>2</sup>\n",
    "    That is, do NOT use a Matlab Toolbox or any other library function, to generate the distributions above directly from the parameters in part a). You MUST do a ‚Äúdewhitening‚Äù instead. In that case, the following Matlab functions can still be useful for this assignment: <code>randn(), peaks(), meshgrid(), surf(), and mesh()</code>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fcbd3699924ae7a730030c0fc04cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07640764 -1.3044661 ]\n",
      "[ 7.07640764 -0.3044661 ]\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sample_colors = (\"tab:blue\", \"tab:pink\")\n",
    "mean_colors = (\"b\", \"m\")\n",
    "\n",
    "class1_samples = np.random.multivariate_normal(mean=[0, 0], cov=np.identity(2), size=1000)\n",
    "print(class1_samples[0])\n",
    "class1_samples = class1_samples\n",
    "class1_samples += means[0]\n",
    "print(class1_samples[0])\n",
    "\n",
    "# x = np.arange(samples[:, 0].min(), samples[:, 0].max(), 0.1)\n",
    "# y = np.arange(samples[:, 1].min(), samples[:, 1].max(), 0.1)\n",
    "# X, Y = np.meshgrid(x, y)\n",
    "# Z = np.ones_like(X)\n",
    "# Z = discriminant_fx(samples, )\n",
    "\n",
    "# surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, antialiased=True)\n",
    "\n",
    "# ax.scatter(viz_X1[:, 0], viz_X1[:, 1], label=\"$\\omega_1$\", c=sample_colors[0])\n",
    "# ax.scatter(viz_X2[:, 0], viz_X2[:, 1], label=\"$\\omega_2$\", c=sample_colors[1])\n",
    "# ax.scatter(means[0, 0], means[0, 1], marker=\"*\", s=250, label=\"$\\mu_1$\", c=mean_colors[0])\n",
    "# ax.scatter(means[1, 0], means[1, 1], marker=\"*\", s=250, label=\"$\\mu_2$\", c=mean_colors[1])\n",
    "# ax.scatter(x[0, 0], x[0, 1], marker=\"x\", color=\"red\", s=100, label=\"Test sample 1\")\n",
    "# ax.scatter(x[1, 0], x[1, 1], marker=\"x\", color=\"magenta\", s=100, label=\"Test sample 2\")\n",
    "\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_zlabel(\"\")\n",
    "\n",
    "# ax.set_aspect(1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6, 3.1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_eigenvalues(cov):\n",
    "    b = - cov[0][0] - cov[1][1]\n",
    "    c = (cov[0][0] * cov[1][1]) - (cov[0][1] * cov[1][0])\n",
    "    x = sympy.Symbol('x')\n",
    "    ans = sympy.solveset((x ** 2) + (b * x) + c, x)\n",
    "    return np.array(list(ans.evalf()), dtype=float)\n",
    "\n",
    "z = compute_eigenvalues(cov[c])\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-check with the example in the lecture note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.6, 3.1],\n",
       "       [2.6, 3.1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues = np.zeros((C, d))\n",
    "for c in range(C):\n",
    "    eigenvalues[c] = compute_eigenvalues(cov[c])\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector $\\phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6, 3.1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvector(cov, eigenvalues):\n",
    "    eigenvectors = np.zeros((len(eigenvalues), len(eigenvalues)))\n",
    "    for i, ev in enumerate(eigenvalues):\n",
    "        denom = (cov[0][0] - eigenvalues[i])\n",
    "        if denom != 0:\n",
    "            a = 1\n",
    "            phi1 = (-cov[0][1] / denom) * a\n",
    "            phi2 = a\n",
    "        else:\n",
    "            phi1 = 1\n",
    "            phi2 = 0\n",
    "        eigenvectors[0, i] = phi1\n",
    "        eigenvectors[1, i] = phi2\n",
    "    return eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_eigenvector(cov[c], eigenvalues[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors = np.zeros((C, d, d))\n",
    "\n",
    "for c in range(C):\n",
    "    for i in range(len(eigenvalues)):\n",
    "        eigenvectors[c] = compute_eigenvector(cov[c], eigenvalues[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.6, 3.1],\n",
       "        [2.6, 3.1]]),\n",
       " array([[[-0.,  1.],\n",
       "         [ 1.,  0.]],\n",
       " \n",
       "        [[-0.,  1.],\n",
       "         [ 1.,  0.]]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.1, 2.6]),\n",
       " array([[1., 0.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(cov[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitened_eigenvectors = np.array([[-1.80424764150708, 1], [0.554247641507076, 1]])\n",
    "\n",
    "# fig = plt.figure(figsize=(18, 4))\n",
    "# cc = 1\n",
    "# ax = fig.add_subplot(140 + cc + 1)\n",
    "# ax.plot([0, whitened_eigenvectors[0, 0]], [0, whitened_eigenvectors[0, 1]])\n",
    "# ax.plot([0, whitened_eigenvectors[1, 0]], [0, whitened_eigenvectors[1, 1]])\n",
    "# ax.set_aspect(1)\n",
    "# ax.set_title(f\"Eigenvectors of the whitened class {cc+1}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle(vector_1, vector_2):\n",
    "    unit_vector_1 = vector_1 / np.linalg.norm(vector_1)\n",
    "    unit_vector_2 = vector_2 / np.linalg.norm(vector_2)\n",
    "    dot_product = np.dot(unit_vector_1, unit_vector_2)\n",
    "    angle_rad = np.arccos(dot_product)\n",
    "    angle = np.rad2deg(angle_rad)\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_random(mean, cov, size):\n",
    "    samples = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "    return (cov @ samples).T + mean\n",
    "\n",
    "N = 1000\n",
    "\n",
    "samples = np.zeros((C, N, d))\n",
    "for c in range(C):\n",
    "    samples[c] = gaussian_random(mean=[0, 0], cov=np.identity(C), size=(C, N))\n",
    "# samples = np.random.multivariate_normal(mean=[0, 0], cov=np.identity(2), size=(2, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1cee491b2d439f8b52a3c7159f75ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "sample_colors = (\"tab:blue\", \"tab:pink\")\n",
    "mean_colors = (\"b\", \"m\")\n",
    "for c in range(C):\n",
    "    ax.scatter(samples[c, :, 0], samples[c, :, 1], label=f\"$\\omega_{c+1}$\", c=sample_colors[c])\n",
    "#     ax.scatter(means[c, 0], means[c, 1], marker=\"*\", s=250, label=f\"$\\mu_{c+1}$\", c=mean_colors[c])\n",
    "ax.set_aspect(1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dewhitening Transformation\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://www.projectrhea.org/rhea/images/thumb/3/35/Fig3_summ_mh.png/685px-Fig3_summ_mh.png!\" /> <br />\n",
    "    <small>https://www.projectrhea.org/rhea/index.php/File:Fig3_summ_mh.png</small>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dewhiten(x, mean, eigenvalues, eigenvectors):\n",
    "    squishing_matrix = np.sqrt(np.identity(len(eigenvalues)) * eigenvalues)\n",
    "#     y = eigenvectors.T @ squishing_matrix @ x\n",
    "    y = squishing_matrix @ x\n",
    "    y = eigenvectors @ y\n",
    "    y += mean\n",
    "    return y\n",
    "\n",
    "# dewhiten(samples[0], means[0], eigenvalues, eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.1, 2.6]),\n",
       " array([[1., 0.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(cov[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.6, 3.1]),\n",
       " array([[-0.,  1.],\n",
       "        [ 1.,  0.]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues[0], eigenvectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dewhitened_samples = np.zeros_like(samples)\n",
    "\n",
    "for c in range(C):\n",
    "    dewhitened_samples[c] = np.array([dewhiten(s, means[c], eigenvalues[c], eigenvectors[c]) for s in samples[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dewhitened_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d732823da30d432b8588fcac26bfe007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "sample_colors = (\"tab:blue\", \"tab:pink\")\n",
    "mean_colors = (\"b\", \"m\")\n",
    "for c in range(C):\n",
    "    ax.scatter(dewhitened_samples[c, :, 0], dewhitened_samples[c, :, 1], label=f\"$\\omega_{c+1}$\", c=sample_colors[c])\n",
    "    ax.scatter(means[c, 0], means[c, 1], marker=\"*\", s=250, label=f\"$\\mu_{c+1}$\", c=mean_colors[c])\n",
    "ax.set_aspect(1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array([[1, 2], [4, 4]])\n",
    "eigenvectors[c] * np.sqrt(np.identity(len(eigenvalues[c])) * eigenvalues[c]) @ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.5589679068359636, 12.695636739787226)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dewhitened_samples[0].min(), dewhitened_samples[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-8eb2d2a3c3a1>:1: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  1./cov[c]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.32258065,        inf],\n",
       "       [       inf, 0.38461538]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1./cov[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "\n",
    "Discriminant function:\n",
    "$$ g_i(x) = -\\frac{1}{2}(x-\\mu_i)^t\\Sigma_i^{-1}(x-\\mu_i)-\\frac{d}{2}\\ln (2\\pi) - \\frac{1}{2} \\ln |\\Sigma_i| + \\ln P(\\omega_i) $$\n",
    "\n",
    "Decision boundary:\n",
    "$$ g_i(x) = g_j(x) $$\n",
    "$$ -\\frac{1}{2}(x-\\mu_i)^t\\Sigma_i^{-1}(x-\\mu_i)-\\frac{d}{2}\\ln (2\\pi) - \\frac{1}{2} \\ln |\\Sigma_i| + \\ln P(\\omega_i) = -\\frac{1}{2}(x-\\mu_j)^t\\Sigma_j^{-1}(x-\\mu_j)-\\frac{d}{2}\\ln (2\\pi) - \\frac{1}{2} \\ln |\\Sigma_j| + \\ln P(\\omega_j) $$\n",
    "$$ -\\frac{1}{2}(x-\\mu_i)^t\\Sigma_i^{-1}(x-\\mu_i) - \\frac{1}{2} \\ln |\\Sigma_i| + \\ln P(\\omega_i) = -\\frac{1}{2}(x-\\mu_j)^t\\Sigma_j^{-1}(x-\\mu_j) - \\frac{1}{2} \\ln |\\Sigma_j| + \\ln P(\\omega_j) $$\n",
    "From $\\Sigma_1 = \\Sigma_2 = \\begin{bmatrix} 3.1 & 0 \\\\ 0 & 2.6 \\end{bmatrix}$,\n",
    "$$ -\\frac{1}{2}(x-\\mu_i)^t\\Sigma_i^{-1}(x-\\mu_i) + \\ln P(\\omega_i) = -\\frac{1}{2}(x-\\mu_j)^t\\Sigma_j^{-1}(x-\\mu_j) + \\ln P(\\omega_j) $$\n",
    "$$ -\\frac{1}{2}\\left[ x^{\\mathsf{T}}\\Sigma_i^{-1} x - 2\\mu_i^{\\mathsf{T}}\\Sigma_i^{-1}x + \\mu_i^{\\mathsf{T}}\\Sigma_i^{-1}\\mu_i \\right] + \\ln P(\\omega_i) = -\\frac{1}{2}\\left[ x^{\\mathsf{T}}\\Sigma_j^{-1} x - 2\\mu_j^{\\mathsf{T}}\\Sigma_j^{-1}x + \\mu_j^{\\mathsf{T}}\\Sigma_j^{-1}\\mu_j \\right] + \\ln P(\\omega_j) $$\n",
    "$$ -\\frac{1}{2} x^{\\mathsf{T}}\\Sigma_i^{-1} x +\\frac{1}{2} 2\\mu_i^{\\mathsf{T}}\\Sigma_i^{-1}x -\\frac{1}{2}\\mu_i^{\\mathsf{T}}\\Sigma_i^{-1}\\mu_i + \\ln P(\\omega_i) = -\\frac{1}{2} x^{\\mathsf{T}}\\Sigma_j^{-1} x + \\frac{1}{2} 2\\mu_j^{\\mathsf{T}}\\Sigma_j^{-1}x -\\frac{1}{2} \\mu_j^{\\mathsf{T}}\\Sigma_j^{-1}\\mu_j + \\ln P(\\omega_j) $$\n",
    "$$ \\frac{1}{2} 2\\mu_i^{\\mathsf{T}}\\Sigma_i^{-1}x -\\frac{1}{2}\\mu_i^{\\mathsf{T}}\\Sigma_i^{-1}\\mu_i + \\ln P(\\omega_i) = \\frac{1}{2} 2\\mu_j^{\\mathsf{T}}\\Sigma_j^{-1}x -\\frac{1}{2} \\mu_j^{\\mathsf{T}}\\Sigma_j^{-1}\\mu_j + \\ln P(\\omega_j) $$\n",
    "$$ \\mu_i^{\\mathsf{T}}\\Sigma_i^{-1}x -\\frac{1}{2}\\mu_i^{\\mathsf{T}}\\Sigma_i^{-1}\\mu_i + \\ln P(\\omega_i) = \\mu_j^{\\mathsf{T}}\\Sigma_j^{-1}x -\\frac{1}{2} \\mu_j^{\\mathsf{T}}\\Sigma_j^{-1}\\mu_j + \\ln P(\\omega_j) $$\n",
    "Let $i=1$ and $j=2$, and from $P(\\omega_1) = 4P(\\omega_2)$, we get\n",
    "$$ \\mu_i^{\\mathsf{T}}\\Sigma_1^{-1}x -\\frac{1}{2}\\mu_1^{\\mathsf{T}}\\Sigma_1^{-1}\\mu_1 + \\ln 4P(\\omega_2) = \\mu_2^{\\mathsf{T}}\\Sigma_2^{-1}x -\\frac{1}{2} \\mu_2^{\\mathsf{T}}\\Sigma_2^{-1}\\mu_2 + \\ln P(\\omega_2) $$\n",
    "$$ \\mu_i^{\\mathsf{T}}\\Sigma_1^{-1}x -\\frac{1}{2}\\mu_1^{\\mathsf{T}}\\Sigma_1^{-1}\\mu_1 + \\ln 4P(\\omega_2) - \\ln P(\\omega_2) = \\mu_2^{\\mathsf{T}}\\Sigma_2^{-1}x -\\frac{1}{2} \\mu_2^{\\mathsf{T}}\\Sigma_2^{-1}\\mu_2 $$\n",
    "$$ \\mu_i^{\\mathsf{T}}\\Sigma_1^{-1}x -\\frac{1}{2}\\mu_1^{\\mathsf{T}}\\Sigma_1^{-1}\\mu_1 + \\ln \\frac{4P(\\omega_2)}{P(\\omega_2)} = \\mu_2^{\\mathsf{T}}\\Sigma_2^{-1}x -\\frac{1}{2} \\mu_2^{\\mathsf{T}}\\Sigma_2^{-1}\\mu_2 $$\n",
    "$$ \\mu_i^{\\mathsf{T}}\\Sigma_1^{-1}x -\\frac{1}{2}\\mu_1^{\\mathsf{T}}\\Sigma_1^{-1}\\mu_1 + \\ln 4 = \\mu_2^{\\mathsf{T}}\\Sigma_2^{-1}x -\\frac{1}{2} \\mu_2^{\\mathsf{T}}\\Sigma_2^{-1}\\mu_2 $$\n",
    "From $\\mu_1 = \\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix}$, $\\mu_2 = \\begin{bmatrix} 1 \\\\ 7 \\end{bmatrix}$,\n",
    "From $\\Sigma_1 = \\Sigma_2 = \\begin{bmatrix} 3.1 & 0 \\\\ 0 & 2.6 \\end{bmatrix}$,\n",
    "$$ \\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} 3.1 & 0 \\\\ 0 & 2.6 \\end{bmatrix}^{-1}\\vec{x} -\\frac{1}{2}\\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} 3.1 & 0 \\\\ 0 & 2.6 \\end{bmatrix}^{-1}\\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix} + \\ln 4 = \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} 3.1 & 0 \\\\ 0 & 2.6 \\end{bmatrix}^{-1}\\vec{x} -\\frac{1}{2} \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} 3.1 & 0 \\\\ 0 & 2.6 \\end{bmatrix}^{-1}\\begin{bmatrix} 1 \\\\ 7 \\end{bmatrix} $$\n",
    "$$ \\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\vec{x} -\\frac{1}{2}\\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix} + \\ln 4 = \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\vec{x} -\\frac{1}{2} \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 7 \\end{bmatrix} $$\n",
    "$$ \\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} -\\frac{1}{2}\\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix} + \\ln 4 = \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} -\\frac{1}{2} \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 7 \\end{bmatrix} $$\n",
    "$$ \\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} -\\frac{1}{2}\\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{7}{3.1} \\\\ \\frac{1}{2.6} \\end{bmatrix} + \\ln 4 \n",
    "= \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} -\\frac{1}{2} \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} \\\\  \\frac{7}{2.6} \\end{bmatrix} $$\n",
    "$$ \\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} -\\frac{1}{2} (\\frac{7 \\cdot 7}{3.1} + \\frac{1}{2.6}) + \\ln 4 \n",
    "= \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} -\\frac{1}{2} (\\frac{1}{3.1} + \\frac{7 \\cdot 7}{2.6}) $$\n",
    "$$ \\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} - 8.0955 + \\ln 4 \n",
    "= \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} - 9.5844 $$\n",
    "$$ \\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + 2.8752\n",
    "= \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{3.1} & 0 \\\\ 0 & \\frac{1}{2.6} \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} $$\n",
    "$$ \\begin{bmatrix} 7 & 1 \\end{bmatrix}\\begin{bmatrix} \\frac{x_1}{3.1} \\\\ \\frac{x_2}{2.6} \\end{bmatrix} + 2.8752\n",
    "= \\begin{bmatrix} 1 & 7 \\end{bmatrix}\\begin{bmatrix} \\frac{x_1}{3.1} \\\\ \\frac{x_2}{2.6} \\end{bmatrix} $$\n",
    "$$ \\frac{7 \\cdot x_1}{3.1} + \\frac{x_2}{2.6} + 2.8752\n",
    "= \\frac{x_1}{3.1} + \\frac{7 \\cdot x_2}{2.6} $$\n",
    "\n",
    "Multiply both sides by $3.1 \\times 2.6 = 8.06$,\n",
    "$$ (2.6 \\cdot 7 \\cdot x_1) + (3.1 \\cdot x_2) + (8.06 \\cdot 2.8752) = (2.6 \\cdot x_1) + (3.1 \\cdot 7 \\cdot x_2) $$\n",
    "$$ 18.2 x_1 + 3.1x_2 + 23.1741 = 2.6 x_1 + 21.7 x_2 $$\n",
    "$$ 15.6 x_1 + 23.1741 = 18.6 x_2 $$\n",
    "$$ 0.8387x_1 + 1.2459 = x_2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(x1):\n",
    "    return (0.8387 * x1) + 1.2459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9398f8107d864327b0ccc22cae33cae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surface_colors = (\"Blues\", \"pink\")\n",
    "mean_colors = (\"b\", \"m\")\n",
    "\n",
    "OFFSET = -10\n",
    "\n",
    "for c in range(C):\n",
    "#     x = np.arange(-10, 12, .1)\n",
    "#     y = np.arange(-4, 16, .1)\n",
    "    x = np.arange(dewhitened_samples[c, :, 0].min(), dewhitened_samples[c, :, 0].max(), .1)\n",
    "    y = np.arange(dewhitened_samples[c, :, 1].min(), dewhitened_samples[c, :, 1].max(), .1)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    zs = np.array([discriminant_fx(s, means[c], cov[c], priors[c]) for s in np.array(list(zip(np.ravel(X), np.ravel(Y))))])\n",
    "    Z = zs.reshape(X.shape)\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, antialiased=True, cmap=surface_colors[c])\n",
    "    \n",
    "    ax.scatter(means[c, 0], means[c, 1], OFFSET, marker=\"*\", s=250, label=f\"$\\mu_{c+1}$\", c=mean_colors[c])\n",
    "    \n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "    major_idx = np.argmax(eigenvalues[c])\n",
    "#         minor_idx = np.argmin(eigenvalues[c])\n",
    "    # HOTFIX: argmax = argmin causes a problem\n",
    "    minor_idx = 1 if major_idx == 0 else 0\n",
    "    alpha = np.arctan2(eigenvectors[c, 1, major_idx], eigenvectors[c, 0, major_idx])\n",
    "    major_r_x = eigenvalues[c, major_idx]\n",
    "    major_r_y = eigenvalues[c, minor_idx]\n",
    "    AXIS_COLORS = [\"black\", \"grey\"]\n",
    "    axis_colors = [AXIS_COLORS[major_idx], AXIS_COLORS[minor_idx]]\n",
    "    \n",
    "    e_X = major_r_x * np.cos(theta) * np.cos(alpha) - major_r_y * np.sin(theta) * np.sin(alpha) + means[c, 0]\n",
    "    e_Y = major_r_x * np.cos(theta) * np.sin(alpha) + major_r_y * np.sin(theta) * np.cos(alpha) + means[c, 1]\n",
    "    e_Z = np.zeros_like(e_X) + OFFSET\n",
    "    ax.plot(e_X, e_Y, e_Z, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "x_min, x_max = ax.get_xlim()\n",
    "x = np.linspace(x_min, x_max, 100)\n",
    "y = decision_boundary(x)\n",
    "ax.plot(x, y, OFFSET, label=\"Decision boundary\", color=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.set_zlabel(\"\")\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = np.array([discriminant_fx(s, means[0], cov[0], priors[0]) for s in np.array(list(zip(np.ravel(X), np.ravel(Y))))])\n",
    "Z = zs.reshape(X.shape)\n",
    "# g1 = np.array([discriminant_fx(s, means[0], cov, priors[0]) for s in dewhitened_samples_c1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ravel(X)[0:3], np.ravel(Y)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(zip(np.ravel(X), np.ravel(Y)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
